import os
from tqdm import tqdm
import openai
from openai import OpenAI, AsyncOpenAI
import asyncio
import time

# Windowsã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã‚’å›é¿
# main.pyã§æ—¢ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ç’°å¢ƒå¤‰æ•°ã®ã¿è¨­å®š
if os.name == 'nt':  # Windows
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
try:
    from transformers import pipeline
except ImportError:
    pipeline = None

try:
    from vllm import LLM, SamplingParams
    from vllm.sampling_params import GuidedDecodingParams
except ImportError:
    LLM = None
    SamplingParams = None
    GuidedDecodingParams = None

try:
    import numpy as np
except ImportError:
    np = None

try:
    import torch
except ImportError:
    torch = None

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenvãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

openai_key = os.getenv('OPENAI_API_KEY')

# è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆè¤‡æ•°ã®æ–¹æ³•ã«å¯¾å¿œï¼‰
openai_keys = []

# æ–¹æ³•1: OPENAI_API_KEYSï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
openai_keys_str = os.getenv('OPENAI_API_KEYS', '')
if openai_keys_str:
    # æ”¹è¡ŒåŒºåˆ‡ã‚Šã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š
    if '\n' in openai_keys_str:
        openai_keys = [key.strip() for key in openai_keys_str.split('\n') if key.strip()]
    else:
        openai_keys = [key.strip() for key in openai_keys_str.split(',') if key.strip()]

# æ–¹æ³•2: OPENAI_API_KEY_1, OPENAI_API_KEY_2, ... ã®å½¢å¼
if not openai_keys:
    api_key_index = 1
    while True:
        key = os.getenv(f'OPENAI_API_KEY_{api_key_index}')
        if key:
            openai_keys.append(key.strip())
            api_key_index += 1
        else:
            break

# æ–¹æ³•3: å˜ä¸€ã®OPENAI_API_KEY
if not openai_keys and openai_key:
    openai_keys = [openai_key]
local_api_url = os.getenv('LOCAL_API_URL', 'http://localhost:1234/v1')
local_api_key = os.getenv('LOCAL_API_KEY', 'lm-studio')
local_model_name = os.getenv('LOCAL_MODEL_NAME', 'qwen/qwen3-vl-30b')


# map each term in text to word_id
def get_vocab_idx(split_text: str, tok_lens):

	vocab_idx = {}
	start = 0

	for w in split_text:
		# print(w, start, start + len(tok_lens[w]))
		if w not in vocab_idx:
			vocab_idx[w] = []

		vocab_idx[w].extend(np.arange(start, start + len(tok_lens[w])))

		start += len(tok_lens[w])

	return vocab_idx

def get_hidden_states(encoded, data_idx, model, layers, static_emb):
	"""Push input IDs through model. Stack and sum `layers` (last four by default).
	Select only those subword token outputs that belong to our word of interest
	and average them."""
	with torch.no_grad():
		output = model(**encoded)

	# Get all hidden states
	states = output.hidden_states
	# Stack and sum all requested layers
	output = torch.stack([states[i] for i in layers]).sum(0).squeeze()

	# Only select the tokens that constitute the requested word

	for w in data_idx:
		static_emb[w] += output[data_idx[w]].sum(dim=0).cpu().numpy()

def chunkify(text, token_lens, length=512):
	chunks = [[]]
	split_text = text.split()
	count = 0
	for word in split_text:
		new_count = count + len(token_lens[word]) + 2 # 2 for [CLS] and [SEP]
		if new_count > length:
			chunks.append([word])
			count = len(token_lens[word])
		else:
			chunks[len(chunks) - 1].append(word)
			count = new_count
	
	return chunks

def constructPrompt(args, init_prompt, main_prompt):
	# GPT APIã¨ãƒ­ãƒ¼ã‚«ãƒ«APIï¼ˆLM Studio/Ollamaï¼‰ã¯åŒã˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã‚’ä½¿ç”¨
	if (args.llm == 'gpt' or args.llm == 'local'):
		return [
            {"role": "system", "content": init_prompt},
            {"role": "user", "content": main_prompt}]
	else:
		# vLLMãªã©ã®å ´åˆã¯æ–‡å­—åˆ—å½¢å¼
		return init_prompt + "\n\n" + main_prompt

def initializeLLM(args):
	args.client = {}

	# vLLMã®åˆæœŸåŒ–ï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
	if LLM is not None and args.llm == 'vllm':
		try:
			args.client['vllm'] = LLM(model="meta-llama/Meta-Llama-3.1-8B-Instruct", tensor_parallel_size=4, gpu_memory_utilization=0.95, 
							   max_num_batched_tokens=4096, max_num_seqs=1000, enable_prefix_caching=True)
		except Exception as e:
			print(f"è­¦å‘Š: vLLMã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
			print("GPTã‚’ä½¿ç”¨ã—ã¾ã™")
			args.llm = 'gpt'

	if args.llm == 'gpt':
		# è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆ
		if hasattr(args, 'openai_api_keys') and args.openai_api_keys:
			api_keys = args.openai_api_keys
		elif openai_keys:
			api_keys = openai_keys
		elif openai_key:
			api_keys = [openai_key]
		else:
			raise ValueError("OPENAI_API_KEYã¾ãŸã¯OPENAI_API_KEYSç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
		
		# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã€å„ã‚­ãƒ¼ã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
		if len(api_keys) > 1:
			args.client[args.llm] = [OpenAI(api_key=key) for key in api_keys]
			args.openai_api_keys = api_keys
			print(f"âœ“ ä¸¦åˆ—å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {len(api_keys)}å€‹ã®APIã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
			for i, key in enumerate(api_keys, 1):
				# APIã‚­ãƒ¼ã®æœ€åˆã¨æœ€å¾Œã®æ•°æ–‡å­—ã ã‘è¡¨ç¤ºï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãŸã‚ï¼‰
				masked_key = f"{key[:7]}...{key[-4:]}" if len(key) > 11 else "***"
				print(f"  - APIã‚­ãƒ¼ {i}: {masked_key}")
		else:
			args.client[args.llm] = OpenAI(api_key=api_keys[0])
			args.openai_api_keys = api_keys
	elif args.llm == 'local':
		# ãƒ­ãƒ¼ã‚«ãƒ«APIï¼ˆLM Studio/Ollamaï¼‰ã®è¨­å®š
		api_url = args.local_api_url if hasattr(args, 'local_api_url') and args.local_api_url else local_api_url
		api_key = args.local_api_key if hasattr(args, 'local_api_key') and args.local_api_key else local_api_key
		model_name = args.local_model_name if hasattr(args, 'local_model_name') and args.local_model_name else local_model_name
		
		args.client['local'] = OpenAI(
			base_url=api_url,
			api_key=api_key
		)
		args.local_model_name = model_name
		print(f"Local API: {api_url}")
		print(f"Model: {model_name}")
	
	return args

async def _make_api_request(async_client, create_params, idx, total, api_key_index=None):
	"""å˜ä¸€ã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’éåŒæœŸã§å®Ÿè¡Œ"""
	try:
		response = await async_client.chat.completions.create(**create_params)
		return (idx, response.choices[0].message.content, None, api_key_index, None)
	except Exception as e:
		# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å¾…æ©Ÿæ™‚é–“ã‚’æŠ½å‡º
		wait_time = None
		if isinstance(e, openai.RateLimitError):
			error_message = str(e)
			# "Please try again in X.XXXs" ã®å½¢å¼ã‹ã‚‰å¾…æ©Ÿæ™‚é–“ã‚’æŠ½å‡º
			import re
			match = re.search(r'try again in ([\d.]+)s', error_message, re.IGNORECASE)
			if match:
				wait_time = float(match.group(1))
		return (idx, None, e, api_key_index, wait_time)

async def promptGPT_parallel_async(args, prompts, api_keys, model_name, schema, max_new_tokens, json_mode, temperature, top_p):
	"""è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’ä½¿ã£ã¦ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆéåŒæœŸç‰ˆï¼‰"""
	from itertools import cycle
	
	# éåŒæœŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
	async_clients = [AsyncOpenAI(api_key=key) for key in api_keys]
	client_cycle = cycle(range(len(async_clients)))  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å¾ªç’°
	
	outputs = [None] * len(prompts)
	
	# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
	create_params_list = []
	for messages in prompts:
		create_params = {
			'model': model_name,
			'stream': False,
			'messages': messages,
			'temperature': temperature,
			'top_p': top_p,
		}
		
		if max_new_tokens > 0:
			create_params['max_tokens'] = max_new_tokens
		
		if json_mode:
			create_params['response_format'] = {"type": "json_object"}
		
		create_params_list.append(create_params)
	
	print(f"\n{'='*60}")
	print(f"ğŸš€ ä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
	print(f"   - ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {len(prompts)}ä»¶")
	print(f"   - APIã‚­ãƒ¼æ•°: {len(api_keys)}å€‹")
	print(f"   - ãƒ¢ãƒ‡ãƒ«: {model_name}")
	print(f"   - æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {max_new_tokens}")
	print(f"{'='*60}")
	
	# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’è¦ç´„ã—ã¦è¡¨ç¤ºï¼ˆæœ€åˆã®3ä»¶ã®ã¿ï¼‰
	if len(prompts) > 0:
		print(f"\nğŸ“ å‡¦ç†å†…å®¹ã®ä¾‹ï¼ˆæœ€åˆã®3ä»¶ï¼‰:")
		for i in range(min(3, len(prompts))):
			messages = prompts[i]
			user_content = messages[-1]['content'] if messages and isinstance(messages[-1], dict) else str(messages)[:100]
			preview = user_content[:150] + "..." if len(user_content) > 150 else user_content
			print(f"   [{i+1}] {preview}")
		if len(prompts) > 3:
			print(f"   ... ä»– {len(prompts) - 3}ä»¶")
	
	# ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ï¼ˆç§’ï¼‰ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.5ç§’
	request_interval = float(os.getenv('OPENAI_REQUEST_INTERVAL', '1.5'))
	
	print(f"\nâ³ ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­ï¼ˆåŒæ™‚å®Ÿè¡Œæ•°: {len(api_keys)}ä»¶ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«: {request_interval}ç§’ï¼‰...")
	
	# ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ã£ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’APIã‚­ãƒ¼æ•°ã«åˆ¶é™
	semaphore = asyncio.Semaphore(len(api_keys))
	start_time = time.time()
	results_dict = {}  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ¼ã¨ã—ã¦çµæœã‚’ä¿å­˜
	rate_limit_errors = []  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
	
	# å„APIã‚­ãƒ¼ã”ã¨ã®æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»ã‚’è¨˜éŒ²
	last_request_time = {i: 0.0 for i in range(len(api_keys))}
	# å„APIã‚­ãƒ¼ã”ã¨ã®ãƒ­ãƒƒã‚¯ï¼ˆåŒã˜APIã‚­ãƒ¼ã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ãï¼‰
	api_key_locks = {i: asyncio.Lock() for i in range(len(api_keys))}
	
	async def process_with_semaphore(idx, create_params, api_key_idx):
		"""ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ã£ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ã—ãªãŒã‚‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
		async with semaphore:
			# åŒã˜APIã‚­ãƒ¼ã¸ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ã
			async with api_key_locks[api_key_idx]:
				# æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ã®çµŒéæ™‚é–“ã‚’ç¢ºèª
				elapsed_since_last = time.time() - last_request_time[api_key_idx]
				if elapsed_since_last < request_interval:
					# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«æ™‚é–“ãŒçµŒéã—ã¦ã„ãªã„å ´åˆã¯å¾…æ©Ÿ
					wait_time = request_interval - elapsed_since_last
					await asyncio.sleep(wait_time)
				
				# ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
				async_client = async_clients[api_key_idx]
				result = await _make_api_request(async_client, create_params, idx, len(prompts), api_key_idx + 1)
				
				# æœ€å¾Œã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»ã‚’æ›´æ–°
				last_request_time[api_key_idx] = time.time()
				
				return result
	
	# ã™ã¹ã¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¿ã‚¹ã‚¯ã¨ã—ã¦ä½œæˆï¼ˆã‚»ãƒãƒ•ã‚©ã§åˆ¶é™ã•ã‚Œã‚‹ï¼‰
	tasks = []
	for idx, create_params in enumerate(create_params_list):
		api_key_idx = next(client_cycle)  # ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ã§APIã‚­ãƒ¼ã‚’é¸æŠ
		tasks.append(process_with_semaphore(idx, create_params, api_key_idx))
	
	# ã‚¿ã‚¹ã‚¯ã‚’é †åºã‚’ä¿è¨¼ã—ã¦å‡¦ç†ï¼ˆgatherã‚’ä½¿ç”¨ã—ã¦é †åºã‚’ä¿è¨¼ï¼‰
	# é€²æ—è¡¨ç¤ºã®ãŸã‚ã€æˆåŠŸã—ãŸã‚¿ã‚¹ã‚¯ã®ã¿ã‚’è¿½è·¡
	success_tasks = set()
	
	# ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
	results_list = await asyncio.gather(*tasks, return_exceptions=True)
	
	# çµæœã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«å‡¦ç†ï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
	for idx, result in enumerate(results_list):
		if isinstance(result, Exception):
			raise result
		idx_result, content, error, api_key_idx, wait_time = result
		results_dict[idx_result] = (idx_result, content, error, api_key_idx, wait_time)
		
		# æˆåŠŸã—ãŸã‚¿ã‚¹ã‚¯ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
		if error is None:
			success_tasks.add(idx_result)
			success_count = len(success_tasks)
			elapsed = time.time() - start_time
			rate = success_count / elapsed if elapsed > 0 else 0
			remaining = len(prompts) - success_count
			eta = remaining / rate if rate > 0 else 0
			print(f"  âœ“ [{success_count:4d}/{len(prompts)}] æˆåŠŸ ({elapsed:.1f}ç§’çµŒé, æ®‹ã‚Šç´„{eta:.1f}ç§’)", end='\r')
		elif isinstance(error, openai.RateLimitError):
			# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
			rate_limit_errors.append((idx_result, api_key_idx, wait_time, error))
	
	print()  # æ”¹è¡Œ
	
	# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†ï¼ˆè¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
	max_retries = int(os.getenv('OPENAI_MAX_RETRIES', '3'))  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3å›ï¼‰
	retry_attempt = 0
	failed_requests = rate_limit_errors.copy()  # å¤±æ•—ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¿½è·¡
	
	while failed_requests and retry_attempt < max_retries:
		retry_attempt += 1
		print(f"\nâš ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒ{len(failed_requests)}ä»¶ç™ºç”Ÿã—ã¾ã—ãŸï¼ˆãƒªãƒˆãƒ©ã‚¤ {retry_attempt}/{max_retries}ï¼‰")
		
		# æœ€å¤§ã®å¾…æ©Ÿæ™‚é–“ã‚’å–å¾—
		max_wait_time = max([w for _, _, w, _ in failed_requests if w is not None], default=5.0)
		if max_wait_time:
			print(f"   â³ {max_wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
			await asyncio.sleep(max_wait_time + 1)  # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
		
		# å¤±æ•—ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒªãƒˆãƒ©ã‚¤
		print(f"   ğŸ”„ {len(failed_requests)}ä»¶ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
		retry_tasks = []
		for idx, api_key_idx, _, error in failed_requests:
			# åˆ¥ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤ï¼ˆãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ï¼‰
			retry_key_idx = next(client_cycle)
			retry_params = create_params_list[idx]
			# ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ã£ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
			retry_tasks.append(process_with_semaphore(idx, retry_params, retry_key_idx))
		
		# ãƒªãƒˆãƒ©ã‚¤ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆã‚»ãƒãƒ•ã‚©ã§åˆ¶é™ã•ã‚Œã‚‹ã€é †åºã‚’ä¿è¨¼ï¼‰
		retry_results = await asyncio.gather(*retry_tasks, return_exceptions=True)
		
		# ãƒªãƒˆãƒ©ã‚¤çµæœã‚’å‡¦ç†
		failed_requests = []  # æ¬¡ã®ãƒªãƒˆãƒ©ã‚¤ç”¨ã«ãƒªã‚»ãƒƒãƒˆ
		retry_success_count = 0
		for retry_result in retry_results:
			if isinstance(retry_result, Exception):
				raise retry_result
			idx, content, error, api_key_idx, wait_time = retry_result
			if error is None:
				results_dict[idx] = (idx, content, None, api_key_idx, wait_time)
				retry_success_count += 1
				print(f"  âœ“ ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ [{retry_success_count}/{len(retry_tasks)}] (ä»¶æ•° {idx+1}, APIã‚­ãƒ¼{api_key_idx})", end='\r')
			else:
				# ãƒªãƒˆãƒ©ã‚¤ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯æ¬¡ã®ãƒªãƒˆãƒ©ã‚¤ã«è¿½åŠ 
				if isinstance(error, openai.RateLimitError):
					failed_requests.append((idx, api_key_idx, wait_time, error))
				else:
					# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«ä¾‹å¤–ã‚’ç™ºç”Ÿ
					print(f"\nâŒ ãƒªãƒˆãƒ©ã‚¤å¤±æ•—: APIã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}, APIã‚­ãƒ¼ {api_key_idx}): {error}")
					raise error
		print()  # æ”¹è¡Œ
		
		# ã™ã¹ã¦æˆåŠŸã—ãŸå ´åˆã¯ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
		if not failed_requests:
			break
	
	# æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¦ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ç©ºã®JSONã‚’è¿”ã™
	if failed_requests:
		print(f"\nâš ï¸  æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ï¼ˆ{max_retries}å›ï¼‰ã«é”ã—ã¾ã—ãŸã€‚{len(failed_requests)}ä»¶ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯å¤±æ•—ã—ã¾ã—ãŸã€‚")
		for idx, api_key_idx, _, error in failed_requests:
			results_dict[idx] = (idx, "{}", error, api_key_idx, None)
	
	# çµæœã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«ä¸¦ã¹æ›¿ãˆ
	results = [results_dict[i] for i in range(len(prompts))]
	
	# çµæœã‚’å‡¦ç†
	success_count = 0
	error_count = 0
	api_key_usage = {i+1: 0 for i in range(len(api_keys))}  # å„APIã‚­ãƒ¼ã®ä½¿ç”¨å›æ•°ã‚’è¨˜éŒ²
	
	for result in results:
		if result is None:
			continue
		idx, content, error, api_key_idx, wait_time = result
		if error is None:
			outputs[idx] = content
			success_count += 1
			api_key_usage[api_key_idx] = api_key_usage.get(api_key_idx, 0) + 1
		else:
			error_count += 1
			# ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’è¡¨ç¤º
			if isinstance(error, openai.RateLimitError):
				error_msg = str(error)
				# ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ã‚’æŠ½å‡º
				if 'tokens per min' in error_msg.lower():
					print(f"\nâŒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}): TPMåˆ¶é™ã«é”ã—ã¾ã—ãŸ")
				elif 'requests per min' in error_msg.lower():
					print(f"\nâŒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}): RPMåˆ¶é™ã«é”ã—ã¾ã—ãŸ")
				else:
					print(f"\nâŒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}): {error_msg[:200]}")
				# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ç©ºã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œ
				outputs[idx] = "{}"
			else:
				print(f"\nâŒ APIã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}, APIã‚­ãƒ¼ {api_key_idx}): {error}")
				# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«ä¾‹å¤–ã‚’ç™ºç”Ÿ
				print(f"\nâŒ APIã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}, APIã‚­ãƒ¼ {api_key_idx}): {error}")
				raise error
	
	# æœ€çµ‚çµæœã‚’è¡¨ç¤º
	total_time = time.time() - start_time
	print(f"\n{'='*60}")
	print(f"âœ… ä¸¦åˆ—å®Ÿè¡Œå®Œäº†")
	print(f"   - æˆåŠŸ: {success_count}/{len(prompts)}ä»¶")
	if error_count > 0:
		print(f"   - ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
	print(f"   - å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
	print(f"   - å¹³å‡é€Ÿåº¦: {success_count/total_time:.2f}ä»¶/ç§’" if total_time > 0 else "   - å¹³å‡é€Ÿåº¦: N/A")
	print(f"\nğŸ“Š APIã‚­ãƒ¼ä½¿ç”¨çŠ¶æ³:")
	for key_idx, count in sorted(api_key_usage.items()):
		percentage = (count / success_count * 100) if success_count > 0 else 0
		print(f"   - APIã‚­ãƒ¼ {key_idx}: {count}ä»¶ ({percentage:.1f}%)")
	print(f"{'='*60}\n")
	
	# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ã‚º
	for client in async_clients:
		await client.close()
	
	return outputs

def promptGPT_parallel(args, prompts, clients, api_keys, model_name, schema, max_new_tokens, json_mode, temperature, top_p):
	"""è¤‡æ•°ã®APIã‚­ãƒ¼ã‚’ä½¿ã£ã¦ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ï¼ˆåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰"""
	outputs = asyncio.run(promptGPT_parallel_async(
		args, prompts, api_keys, model_name, schema, max_new_tokens, json_mode, temperature, top_p
	))
	return outputs

def promptGPT(args, prompts, schema=None, max_new_tokens=1024, json_mode=True, temperature=0.1, top_p=0.99):
	import time
	import openai
	import json
	import asyncio
	from itertools import cycle
	
	# ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨ãƒ¢ãƒ‡ãƒ«åã‚’æ±ºå®š
	if args.llm == 'local':
		client_key = 'local'
		model_name = getattr(args, 'local_model_name', 'qwen/qwen3-vl-30b')
		desc = "Local LLM API"
		use_parallel = False
		client = args.client[client_key]
	else:
		client_key = 'gpt'
		model_name = 'gpt-4o-mini-2024-07-18'
		desc = "GPT API"
		# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã§ã‚‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ1ä»¶ã®å ´åˆã¯é †æ¬¡å‡¦ç†ã‚’ä½¿ç”¨ï¼ˆSTEP2ãªã©ï¼‰
		# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¤‡æ•°ã‚ã‚‹å ´åˆã®ã¿ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨
		clients = args.client[client_key]
		if isinstance(clients, list) and len(clients) > 1 and len(prompts) > 1:
			use_parallel = True
			api_keys = args.openai_api_keys
			client = None  # ä¸¦åˆ—å‡¦ç†ã§ã¯ä½¿ç”¨ã—ãªã„
		else:
			use_parallel = False
			if isinstance(clients, list):
				client = clients[0]
			else:
				client = clients
	
	# ä¸¦åˆ—å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
	if use_parallel:
		return promptGPT_parallel(args, prompts, clients, api_keys, model_name, schema, max_new_tokens, json_mode, temperature, top_p)
	
	# é †æ¬¡å‡¦ç†ï¼ˆæ—¢å­˜ã®å®Ÿè£…ï¼‰
	outputs = []
	# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã€é †æ¬¡å‡¦ç†ã§ã‚‚åˆ¥ã®ã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
	available_clients = []
	use_multiple_keys = False
	if args.llm == 'gpt' and hasattr(args, 'openai_api_keys') and args.openai_api_keys and len(args.openai_api_keys) > 1:
		# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ
		available_clients = [OpenAI(api_key=key) for key in args.openai_api_keys]
		client_cycle = cycle(range(len(available_clients)))  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å¾ªç’°
		use_multiple_keys = True
		print(f"âœ“ é †æ¬¡å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°APIã‚­ãƒ¼å¯¾å¿œï¼‰: {len(available_clients)}å€‹ã®APIã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ã¾ã™")
	else:
		# å˜ä¸€ã®APIã‚­ãƒ¼ã®å ´åˆ
		available_clients = [client]
		client_cycle = cycle([0])  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹0ã®ã¿
	
	for idx, messages in enumerate(tqdm(prompts, desc=desc, ncols=80, ascii=True)):
		max_retries = len(available_clients) * 3  # å„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
		retry_delay = 1
		current_key_idx = next(client_cycle)
		current_client = available_clients[current_key_idx]
		
		for attempt in range(max_retries):
			try:
				# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ§‹ç¯‰
				create_params = {
					'model': model_name,
					'stream': False,
					'messages': messages,
					'temperature': temperature,
					'top_p': top_p,
				}
				
				# max_tokensã®è¨­å®šï¼ˆ0ã‚ˆã‚Šå¤§ãã„å ´åˆã®ã¿è¨­å®šï¼‰
				if max_new_tokens > 0:
					create_params['max_tokens'] = max_new_tokens
				
				# JSONãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
				# Alibaba Cloud Model Studioã®Qwen APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã«åŸºã¥ãã€
				# OpenAIäº’æ›APIã§ã¯response_formatãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹
				# GPT APIä½¿ç”¨æ™‚: json_object
				# LM StudioçµŒç”±ã®Qwenãƒ¢ãƒ‡ãƒ«: json_schemaï¼ˆã‚¹ã‚­ãƒ¼ãƒå®šç¾©ãŒå¿…è¦ï¼‰
				if json_mode:
					if args.llm == 'local':
						# ãƒ­ãƒ¼ã‚«ãƒ«APIï¼ˆLM Studio/Ollamaï¼‰ä½¿ç”¨æ™‚ã¯json_schemaã‚’ä½¿ç”¨
						# schemaãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€JSONã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ
						if schema is not None:
							try:
								# Pydanticã‚¹ã‚­ãƒ¼ãƒã‹ã‚‰JSONã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ
								json_schema = schema.model_json_schema()
								create_params['response_format'] = {
									"type": "json_schema",
									"json_schema": {
										"name": schema.__name__ if hasattr(schema, '__name__') else "response",
										"strict": True,
										"schema": json_schema
									}
								}
							except Exception as e:
								# ã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§JSONå½¢å¼ã‚’æŒ‡å®š
								print(f"è­¦å‘Š: JSONã‚¹ã‚­ãƒ¼ãƒã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§JSONå½¢å¼ã‚’æŒ‡å®šã—ã¾ã™ã€‚")
								pass
						else:
							# schemaãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã§JSONå½¢å¼ã‚’æŒ‡å®š
							pass
					else:
						# GPT APIä½¿ç”¨æ™‚ã¯json_objectã‚’ä½¿ç”¨
						create_params['response_format'] = {"type": "json_object"}
				
				response = current_client.chat.completions.create(**create_params)
				outputs.append(response.choices[0].message.content)
				break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
			except (openai.AuthenticationError, openai.PermissionDeniedError) as e:
				# APIã‚­ãƒ¼ãŒç„¡åŠ¹ãªå ´åˆã€åˆ¥ã®ã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
				if use_multiple_keys:
					print(f"\nâš ï¸  APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ (ä»¶æ•° {idx+1}/{len(prompts)}, APIã‚­ãƒ¼ {current_key_idx + 1})ã€‚åˆ¥ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
					current_key_idx = next(client_cycle)
					current_client = available_clients[current_key_idx]
					if attempt < max_retries - 1:
						continue
				print(f"\nã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ (ä»¶æ•° {idx+1}/{len(prompts)}): {e}")
				if hasattr(e, 'response') and e.response is not None:
					try:
						error_body = e.response.text
						print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_body}")
					except:
						pass
				raise
			except openai.RateLimitError as e:
				# ãƒ­ãƒ¼ã‚«ãƒ«APIã§ã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã¯ç™ºç”Ÿã—ãªã„ãŒã€äº’æ›æ€§ã®ãŸã‚æ®‹ã™
				# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯åˆ¥ã®ã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
				if use_multiple_keys:
					print(f"\nâš ï¸  ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}/{len(prompts)}, APIã‚­ãƒ¼ {current_key_idx + 1})ã€‚åˆ¥ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
					current_key_idx = next(client_cycle)
					current_client = available_clients[current_key_idx]
					if attempt < max_retries - 1:
						continue
				if attempt < max_retries - 1:
					wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
					print(f"\nè­¦å‘Š: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}/{len(prompts)})ã€‚{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
					time.sleep(wait_time)
				else:
					print(f"\nã‚¨ãƒ©ãƒ¼: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãŒ{max_retries}å›ç¶šãã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
					raise
			except openai.APIError as e:
				# è¤‡æ•°ã®APIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯åˆ¥ã®ã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤
				if use_multiple_keys and attempt < max_retries - 1:
					print(f"\nâš ï¸  APIã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}/{len(prompts)}, APIã‚­ãƒ¼ {current_key_idx + 1}): {e}ã€‚åˆ¥ã®APIã‚­ãƒ¼ã§ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
					current_key_idx = next(client_cycle)
					current_client = available_clients[current_key_idx]
					continue
				if attempt < max_retries - 1:
					wait_time = retry_delay * (2 ** attempt)
					print(f"\nè­¦å‘Š: APIã‚¨ãƒ©ãƒ¼ (ä»¶æ•° {idx+1}/{len(prompts)}): {e}")
					if hasattr(e, 'response') and e.response is not None:
						try:
							error_body = e.response.text
							print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_body}")
						except:
							pass
					print(f"{wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
					time.sleep(wait_time)
				else:
					print(f"\nã‚¨ãƒ©ãƒ¼: APIã‚¨ãƒ©ãƒ¼ãŒ{max_retries}å›ç¶šãã¾ã—ãŸ (ä»¶æ•° {idx+1}/{len(prompts)}): {e}")
					if hasattr(e, 'response') and e.response is not None:
						try:
							error_body = e.response.text
							print(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_body}")
						except:
							pass
					raise
			except Exception as e:
				print(f"\nã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ (ä»¶æ•° {idx+1}/{len(prompts)}): {e}")
				raise
	return outputs

def promptLlamaVLLM(args, prompts, schema=None, max_new_tokens=1024, temperature=0.1, top_p=0.99):
    if LLM is None or SamplingParams is None:
        raise ImportError("vLLMãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GPTã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€vLLMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    if schema is None:
        sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_new_tokens)
    else:
        if GuidedDecodingParams is None:
            raise ImportError("vLLMã®GuidedDecodingParamsãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        guided_decoding_params = GuidedDecodingParams(json=schema.model_json_schema())
        sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_new_tokens, 
                                    guided_decoding=guided_decoding_params)
    generations = args.client['vllm'].generate(prompts, sampling_params)
    
    outputs = []
    for gen in generations:
        outputs.append(gen.outputs[0].text)

    return outputs

def promptLLM(args, prompts, schema=None, max_new_tokens=1024, json_mode=True, temperature=0.1, top_p=0.99):
	if args.llm == 'gpt' or args.llm == 'local':
		return promptGPT(args, prompts, schema, max_new_tokens, json_mode, temperature, top_p)
	else:
		return promptLlamaVLLM(args, prompts, schema, max_new_tokens, temperature, top_p)
	